services:
  # Postgres (for Airflow)
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - churn_task
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  # Airflow services
  airflow-webserver:
    build: ./airflow
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init &&
        airflow users create --role Admin --username admin --password admin --email admin@airflow.com --firstname admin --lastname admin &&
        airflow connections delete 'spark_default' &&
        airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'spark://spark' --conn-port 7077 --conn-extra '{"deploy-mode": "client"}' &&
        airflow webserver
    ports:
      - 8090:8080
    volumes:
      - ./../data:/opt/airflow/data
      - ./../dags:/opt/airflow/dags
      - ./../src:/opt/airflow/src
    networks:
      - churn_task
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 15s
      timeout: 10s
      retries: 5
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-here
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True
    depends_on:
      postgres:
        condition: service_healthy

  airflow-scheduler:
    build: ./airflow
    command: airflow scheduler
    volumes:
      - ./../data:/opt/airflow/data
      - ./../dags:/opt/airflow/dags
      - ./../src:/opt/airflow/src
    networks:
      - churn_task
    depends_on:
      postgres:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-here
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True

  # Hadoop services
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop/config
    networks:
      - churn_task

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop/config
    networks:
      - churn_task

  # MLflow service
  mlflow_server:
    image: ghcr.io/mlflow/mlflow:v2.19.0
    container_name: mlflow-container
    environment:
      - TZ=UTC
      - GIT_PYTHON_REFRESH=quiet
      - MLFLOW_TRACKING_URI=http://0.0.0.0:5000
      - MLFLOW_ARTIFACT_URI=file:///mlflow/artifacts
    ports:
      - "5001:5000"
    networks:
      - churn_task
    command: mlflow ui --host 0.0.0.0
    volumes:
      - mlflow_artifacts:/mlflow/artifacts

  # Spark services
  spark:
    image: docker.io/bitnami/spark:3.5.3
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - '8080:8080'
      - '7077:7077'
    networks:
      - churn_task

  spark-worker:
    image: docker.io/bitnami/spark:3.5.3
    command: bash -c "pip install numpy && /opt/bitnami/scripts/spark/run.sh"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks:
      - churn_task

  spark-worker-2:
    image: docker.io/bitnami/spark:3.5.3
    command: bash -c "pip install numpy && /opt/bitnami/scripts/spark/run.sh"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks:
      - churn_task

volumes:
  hadoop_namenode:
  hadoop_datanode:
  mlflow_artifacts:

networks:
  churn_task:
    name: churn_network 